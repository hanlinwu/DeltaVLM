# Evaluation Configuration for DeltaVLM
# Usage: python scripts/evaluate.py --cfg_path configs/evaluate.yaml

model:
  arch: "instruct_vicuna7b"
  model_type: "vicuna7b"
  
  # NOTE: Set this to your trained checkpoint path
  pretrained: "./output/stage2/checkpoint_best.pth"
  
  vit_model: "eva_clip_g"
  image_size: 224
  vit_precision: "fp16"
  freeze_vit: true
  
  num_query_token: 32
  qformer_text_input: true
  
  llm_model: "lmsys/vicuna-7b-v1.5"
  
  max_txt_len: 128
  max_output_txt_len: 256

datasets:
  changechat:
    type: default
    
    vis_processor:
      eval:
        name: blip_image_eval
        image_size: 224
        
    text_processor:
      eval:
        name: blip_caption
    
    # NOTE: Update these paths to your local dataset location
    build_info:
      annotations:
        test:
          storage: "data/changechat/annotations/test.json"
      images:
        storage: "data/changechat/images"

run:
  task: image_text_pretrain
  
  evaluate: true
  test_splits: ["test"]
  
  batch_size_eval: 8
  num_workers: 4
  
  max_len: 256
  min_len: 10
  num_beams: 5
  
  device: cuda
  distributed: false
  
  output_dir: "output/eval"
